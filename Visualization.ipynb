{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cli_visualization import HuatuoChatbot\n",
    "import torch\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import TextStreamer\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "NUM_IMG_TOKENS = 576\n",
    "PATCHES = 24\n",
    "SIZE = (336,336)\n",
    "\n",
    "bot = HuatuoChatbot(\"FreedomIntelligence/HuatuoGPT-Vision-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def generate_attention_maps(question, image_path, layer=16, query=-1):\n",
    "\n",
    "    general_question = 'Write a general description of the image.'\n",
    "\n",
    "    prompt = f\"{question} Answer the question using a single word or phrase.\"\n",
    "    general_prompt = f\"{general_question} Answer the question using a single word or phrase.\"\n",
    "\n",
    "    model_output, input_ids = bot.inference_with_attention_output(prompt,image_path)\n",
    "    # print(f\"Answer: {response_qs}\")\n",
    "    input_ids = input_ids[0].cpu()\n",
    "    index = torch.where(input_ids==-200)[0]\n",
    "    att_map = model_output['attentions'][layer][0, :, query, index:index+NUM_IMG_TOKENS].mean(dim=0).to(torch.float32).detach().cpu().numpy().reshape(PATCHES, PATCHES)\n",
    "    model_output, input_ids = bot.inference_with_attention_output(general_prompt,image_path)\n",
    "    # print(f\"Description: {response_general}\")\n",
    "    input_ids = input_ids[0].cpu()\n",
    "    index = torch.where(input_ids==-200)[0]\n",
    "    general_att_map = model_output['attentions'][layer][0, :, query, index:index+NUM_IMG_TOKENS].mean(dim=0).to(torch.float32).detach().cpu().numpy().reshape(PATCHES, PATCHES)\n",
    "\n",
    "    return att_map, general_att_map\n",
    "\n",
    "\n",
    "def show_mask_on_image(img, mask):\n",
    "    img = np.float32(img) / 255\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_HSV)\n",
    "    hm = np.float32(heatmap) / 255\n",
    "    cam = hm + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam), heatmap\n",
    "\n",
    "\n",
    "def normalize(img):\n",
    "    return (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "\n",
    "def get_attention_map_from_vit(question, image_path, layer=4):\n",
    "    vit_attention = bot.inference_with_vit_attention(question,image_path)\n",
    "    vit_attention = vit_attention[layer][0, :, 0, 1:].mean(dim=0).to(torch.float32).detach().cpu().numpy().reshape(PATCHES, PATCHES)\n",
    "    return vit_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def convert_bbox_after_padding_and_resize(bbox, original_size, resized_size):\n",
    "\n",
    "    W_orig, H_orig = original_size\n",
    "    W_new, H_new = resized_size\n",
    "\n",
    "    # Compute padding offsets\n",
    "    if W_orig > H_orig:\n",
    "        pad_top = (W_orig - H_orig) // 2\n",
    "        pad_left = 0\n",
    "    elif H_orig > W_orig:\n",
    "        pad_top = 0\n",
    "        pad_left = (H_orig - W_orig) // 2\n",
    "    else:\n",
    "        pad_top = pad_left = 0\n",
    "\n",
    "    W_pad, H_pad = max(W_orig, H_orig), max(W_orig, H_orig)\n",
    "\n",
    "    # Shift bbox due to padding\n",
    "    x_min, y_min, width, height = bbox\n",
    "    x_min_pad = x_min + pad_left\n",
    "    y_min_pad = y_min + pad_top\n",
    "\n",
    "    # Scale from padded to resized\n",
    "    scale_x = W_new / W_pad\n",
    "    scale_y = H_new / H_pad\n",
    "\n",
    "    x_min_new = x_min_pad * scale_x\n",
    "    y_min_new = y_min_pad * scale_y\n",
    "    width_new = width * scale_x\n",
    "    height_new = height * scale_y\n",
    "\n",
    "    return (x_min_new, y_min_new, width_new, height_new)\n",
    "\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith('http://') or image_file.startswith('https://'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image\n",
    "\n",
    "\n",
    "def expand2square(pil_img, background_color):\n",
    "    width, height = pil_img.size\n",
    "    if width == height:\n",
    "        return pil_img\n",
    "    elif width > height:\n",
    "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "        result.paste(pil_img, (0, (width - height) // 2))\n",
    "        return result\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "        return result\n",
    "\n",
    "        \n",
    "def draw_bboxes(image_input, bboxes, name, thickness, folder):\n",
    "\n",
    "    # image = image_input.numpy()\n",
    "    image = np.array(image_input)\n",
    "\n",
    "    if image.max() <= 1.0:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    # Show image with viridis colormap\n",
    "    ax.imshow(image, cmap='viridis')\n",
    "\n",
    "    colors = ['red'] * len(bboxes)\n",
    "\n",
    "    # Draw each bounding box\n",
    "    for bbox, color in zip(bboxes, colors):\n",
    "        x, y, w, h = bbox\n",
    "        rect = patches.Rectangle((x, y), w, h, linewidth=thickness,\n",
    "                                 edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.join(folder, name), exist_ok=True)\n",
    "    plt.savefig(os.path.join(folder, f\"{name}/source.png\"), bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "\n",
    "\n",
    "def draw_att_map(qs, image_path, layers, bboxes, name, thickness, folder):\n",
    "    for layer in layers:\n",
    "        att_map, general_att_map = generate_attention_maps(qs, image_path, layer=layer)\n",
    "        att_map_scaled = normalize(torch.tensor(att_map / general_att_map))\n",
    "        attn_over_image = torch.nn.functional.interpolate(\n",
    "            att_map_scaled.unsqueeze(0).unsqueeze(0), \n",
    "            size=SIZE, \n",
    "            mode='nearest', \n",
    "        ).squeeze()\n",
    "        draw_bboxes_attention_map(attn_over_image, bboxes, name, layer, thickness, folder)\n",
    "\n",
    "\n",
    "def draw_bboxes_attention_map(image_input, bboxes, name, layer, thickness, folder):\n",
    "\n",
    "    image = image_input.numpy()\n",
    "\n",
    "    if image.max() <= 1.0:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    # Show image with viridis colormap\n",
    "    im = ax.imshow(image, cmap='viridis')\n",
    "\n",
    "    colors = ['red'] * len(bboxes)\n",
    "\n",
    "    # Draw each bounding box\n",
    "    for bbox, color in zip(bboxes, colors):\n",
    "        x, y, w, h = bbox\n",
    "        rect = patches.Rectangle((x, y), w, h, linewidth=thickness,\n",
    "                                 edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    # plt.colorbar(im, ax=ax)\n",
    "    plt.savefig(os.path.join(folder, f\"{name}/layer{layer}.png\"), bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_num = 6\n",
    "# layers = [0,15,25,35,45,55]\n",
    "layers = [0,5,10,16,20,27]\n",
    "\n",
    "questions = []\n",
    "# dataset = \"COCO\"\n",
    "dataset = \"SLAKE\"\n",
    "# subset = \"localization\"\n",
    "subset = \"attribute\"\n",
    "input_path = f\"./{dataset}_{subset}_questions.jsonl\"\n",
    "with open(input_path, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        questions.append(json.loads(line))\n",
    "\n",
    "\n",
    "qs_num = 1\n",
    "sample = questions[qs_num]\n",
    "image_path = os.path.join(\"/local_data/local_data/mllm_datasets/evaluation_datasets/slake/imgs\", sample[\"image\"], \"source.jpg\")\n",
    "# image_path = os.path.join('../../local_data/mllm_datasets/evaluation_datasets/coco/val2014', f'COCO_val2014_{sample[\"image\"]:012d}.jpg')\n",
    "bboxes = sample[\"bbox\"]\n",
    "assert len(bboxes) == 1\n",
    "original_image = load_image(image_path)\n",
    "image = expand2square(original_image, 0)\n",
    "image = image.resize((336, 336))\n",
    "bbox = convert_bbox_after_padding_and_resize(bboxes[0], original_image.size, resized_size=(336, 336))\n",
    "print(sample[\"question\"])\n",
    "name = f\"{dataset}_{subset}_{qs_num}\"\n",
    "folder = \"./huatuo34b_samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thickness = 3\n",
    "draw_bboxes(image, [bbox], name, thickness, folder)\n",
    "with open(os.path.join(folder, name, \"question.txt\"), \"w\") as file:\n",
    "    file.write(sample[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_att_map(sample[\"question\"], image_path, layers, [bbox], name, thickness, folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
