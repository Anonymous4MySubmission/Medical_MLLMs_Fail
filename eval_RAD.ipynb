{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/Desktop/code/HuatuoGPT-Vision\n",
      "loading from FreedomIntelligence/HuatuoGPT-Vision-7B\n",
      "[2025-05-10 19:18:15,605] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Downloads/ENTER/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from CLIP first. This should only be used at inference!!!\n",
      "loading vision model from openai/clip-vit-large-patch14-336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Downloads/ENTER/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc11a23fd114aeb9480b307a0fd1b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from cli_new import HuatuoChatbot\n",
    "import torch\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from methods import *\n",
    "\n",
    "\n",
    "NUM_IMG_TOKENS = 576\n",
    "PATCHES = 24\n",
    "SIZE = (336, 336)\n",
    "\n",
    "bot = HuatuoChatbot(\"FreedomIntelligence/HuatuoGPT-Vision-7B\", device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_GT_KnockOut_acc(json_path, output_filename, filename_suffix, masked_layer):\n",
    "\n",
    "    if json_path == \"slake_qa_set\": # skip this, because SLAKE QA has no gt_token\n",
    "        return\n",
    "    \n",
    "    qa_set_attention_maps = np.load(f'./attention_maps/{output_filename}_set_attention_maps{filename_suffix}.npy', allow_pickle=True).item()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    incorrect_ids_qa_ak=[]\n",
    "\n",
    "    for id, qa in tqdm(qa_set_attention_maps.items()):\n",
    "        prompt = f\"{qa['question']} Answer the question using a single word or phrase.\"\n",
    "        \n",
    "        # 2nd pass\n",
    "        # inputs_embeds_shape = input_ids.shape[-1] - 1 + NUM_IMG_TOKENS\n",
    "        input_ids = np.array(qa[\"input_ids\"])\n",
    "        image_start_token_index = np.where(input_ids==-200)[0][0]\n",
    "        gt_tokens = qa[\"gt_token\"]\n",
    "        gt_tokens = [token_index[0]*PATCHES + token_index[1] for token_index in gt_tokens]\n",
    "        all_img_tokens = [idx for idx in range(NUM_IMG_TOKENS)]\n",
    "        inv_gt_tokens = [token for token in all_img_tokens if token not in gt_tokens]\n",
    "        \n",
    "        last_token_index = [idx for idx in range(image_start_token_index + NUM_IMG_TOKENS, input_ids.shape[-1] + NUM_IMG_TOKENS - 1)]\n",
    "        if not isinstance(last_token_index, list):\n",
    "            last_token_index = [last_token_index]\n",
    "\n",
    "        block_config = {}\n",
    "        image_range = [image_start_token_index + patch_index for patch_index in inv_gt_tokens]\n",
    "\n",
    "        block_ids = [image_range, last_token_index]\n",
    "        temp = [(stok1, stok0) for stok0 in block_ids[0] for stok1 in block_ids[1]]\n",
    "\n",
    "        for layer in masked_layer:\n",
    "            block_config[layer] = copy.deepcopy(temp)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            block_attn_hooks = set_block_attn_hooks_llava(bot.model, block_config, block_desc=\"Image->Question\")\n",
    "            _, input_ids, response_qs = bot.inference(prompt, qa['image_path'])\n",
    "            remove_wrapper_llava(bot.model, block_attn_hooks)\n",
    "\n",
    "        answer = qa['answer']\n",
    "\n",
    "        clean_response = response_qs.replace(\"<|endoftext|>\", \"\")\n",
    "\n",
    "        if clean_response in answer or answer in clean_response:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect_ids_qa_ak.append(id)\n",
    "        total += 1\n",
    "\n",
    "    print(f\"GT Accuracy on {json_path.replace('_', ' ')}: {(correct / total) * 100:.4f}%\")\n",
    "\n",
    "    os.makedirs(\"./incorrect\", exist_ok=True)\n",
    "    with open(f\"./incorrect/incorrect_ids_{output_filename}_gt{filename_suffix}.json\", \"w\") as f:\n",
    "        json.dump(incorrect_ids_qa_ak, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import copy\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def convert_bounding_box_to_resized(bboxes, original_size, new_size):\n",
    "    # Extract original image size and resized image size\n",
    "    W_orig, H_orig = original_size\n",
    "    W_new, H_new = new_size\n",
    "\n",
    "    # Compute scaling factors\n",
    "    scale_x = W_new / W_orig\n",
    "    scale_y = H_new / H_orig\n",
    "\n",
    "    # Handle both single bbox and list of bboxes\n",
    "    if not isinstance(bboxes[0], list) and not isinstance(bboxes[0], tuple):\n",
    "        bboxes = [bboxes]\n",
    "    \n",
    "    resized_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        # Extract the bounding box in the original image\n",
    "        x_min, y_min, width, height = bbox\n",
    "\n",
    "        # Convert to new image coordinates\n",
    "        x_min_new = x_min * scale_x\n",
    "        y_min_new = y_min * scale_y\n",
    "        width_new = width * scale_x\n",
    "        height_new = height * scale_y\n",
    "\n",
    "        resized_bboxes.append([x_min_new, y_min_new, width_new, height_new])\n",
    "    \n",
    "    return resized_bboxes\n",
    "\n",
    "\n",
    "def get_tokens_covering_bbox(bboxes, num_tokens=24):\n",
    "\n",
    "    img_width, img_height = SIZE\n",
    "    token_width = img_width / num_tokens\n",
    "    token_height = img_height / num_tokens\n",
    "    \n",
    "    # Convert single bbox to list for uniform processing\n",
    "    if not isinstance(bboxes, list):\n",
    "        bboxes = [bboxes]\n",
    "    \n",
    "    selected_tokens = set()  # Use set to avoid duplicate tokens\n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        x_min, y_min, width, height = bbox\n",
    "        \n",
    "        x_min_token = int(np.floor(x_min / token_width))\n",
    "        y_min_token = int(np.floor(y_min / token_height))\n",
    "        x_max_token = int(np.ceil((x_min + width) / token_width))\n",
    "        y_max_token = int(np.ceil((y_min + height) / token_height))\n",
    "\n",
    "        x_max_token = min(x_max_token, num_tokens - 1)\n",
    "        y_max_token = min(y_max_token, num_tokens - 1)\n",
    "        \n",
    "        ### Note it is (y, x) here\n",
    "        bbox_tokens = [(y, x) for x in range(x_min_token, x_max_token) \n",
    "                               for y in range(y_min_token, y_max_token)]\n",
    "        selected_tokens.update(bbox_tokens)\n",
    "\n",
    "    return list(selected_tokens)\n",
    "\n",
    "def draw_tokens_on_image(img, token_indices, num_tokens=24, color=(0, 255, 0), thickness=1):\n",
    "    # Load the image\n",
    "    h, w = SIZE  # Get image dimensions\n",
    "\n",
    "    # Compute grid cell size\n",
    "    token_width = w / num_tokens\n",
    "    token_height = h / num_tokens\n",
    "    \n",
    "    # Create a copy of the image if it's a tensor\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img_np = img.numpy().copy()\n",
    "    else:\n",
    "        img_np = np.array(img).copy()\n",
    "    \n",
    "    # Make sure the image is in the right format for OpenCV\n",
    "    if len(img_np.shape) == 2:  # If it's a grayscale image\n",
    "        img_np = cv2.cvtColor(img_np.astype(np.float32), cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # Convert to uint8 if it's float\n",
    "    if img_np.dtype == np.float32 or img_np.dtype == np.float64:\n",
    "        img_np = (img_np * 255).astype(np.uint8)\n",
    "\n",
    "    # Already switch x and y here\n",
    "    for y_token, x_token in token_indices:\n",
    "        # Compute the top-left corner pixel coordinates of the token\n",
    "        x_pixel = int(x_token * token_width)\n",
    "        y_pixel = int(y_token * token_height)\n",
    "\n",
    "        # Draw the rectangle (top-left, bottom-right)\n",
    "        cv2.rectangle(img_np, (x_pixel, y_pixel),\n",
    "                      (int((x_token + 1) * token_width), int((y_token + 1) * token_height)),\n",
    "                      color, thickness)\n",
    "\n",
    "    return img_np\n",
    "\n",
    "def normalize(img):\n",
    "    return (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "def show_mask_on_image(img, mask):\n",
    "    img = np.float32(img) / 255\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_HSV)\n",
    "    hm = np.float32(heatmap) / 255\n",
    "    cam = hm + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam), heatmap\n",
    "\n",
    "def visualize_attention_map(att_map, image_path, gt_token=None, threshold=None, mask=None, v_minmax=None, save_image=False, id=None):\n",
    "\n",
    "    att_map_scaled = torch.tensor(att_map)\n",
    "    att_map_scaled = normalize(att_map_scaled)\n",
    "\n",
    "    # To only show the bbox area\n",
    "    if mask and (gt_token is not None):\n",
    "        mask = np.zeros_like(att_map_scaled)\n",
    "        for x, y in gt_token:\n",
    "            mask[x, y] = 1\n",
    "\n",
    "        att_map_scaled = att_map_scaled * mask\n",
    "\n",
    "    attn_over_image = torch.nn.functional.interpolate(\n",
    "        att_map_scaled.unsqueeze(0).unsqueeze(0), \n",
    "        size=(336,336), \n",
    "        mode='nearest', \n",
    "    ).squeeze()\n",
    "\n",
    "    img = Image.open(image_path).resize((336, 336))\n",
    "    np_img = np.array(img)\n",
    "    img_with_attn, _ = show_mask_on_image(np_img, attn_over_image.numpy())\n",
    "\n",
    "    if gt_token is not None:\n",
    "        np_img = draw_tokens_on_image(np_img, gt_token)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "    axes[0].imshow(np_img)\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    if v_minmax is not None:\n",
    "        axes[1].imshow(att_map_scaled, vmin=v_minmax[0], vmax=v_minmax[1])\n",
    "    else:\n",
    "        axes[1].imshow(att_map_scaled)\n",
    "        \n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(img_with_attn)\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if save_image:\n",
    "        sample_id = image_path.split(\"/\")[-2]\n",
    "        # Create numbered output directory\n",
    "        output_dir = os.path.join(\"./output_images\", sample_id, f\"qid{id}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        if v_minmax is not None:\n",
    "            output_filename = os.path.join(output_dir, \"after.png\")\n",
    "        else:\n",
    "            output_filename = os.path.join(output_dir, \"before.png\")\n",
    "\n",
    "        fig.savefig(output_filename, bbox_inches='tight', dpi=300)\n",
    "\n",
    "        original_img_path = os.path.join(output_dir, \"source.png\")\n",
    "        img.save(original_img_path)\n",
    "\n",
    "        att_map_path = output_filename.replace(\".png\", \".npy\")\n",
    "        np.save(att_map_path, att_map_scaled.numpy())\n",
    "\n",
    "        # Save the attention map\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        if v_minmax is not None:\n",
    "            plt.imshow(att_map_scaled, vmin=v_minmax[0], vmax=v_minmax[1])\n",
    "        else:\n",
    "            plt.imshow(att_map_scaled)\n",
    "        plt.axis(\"off\")\n",
    "        att_map_img_path = output_filename.replace(\".png\", \"_att.png\")\n",
    "        plt.savefig(att_map_img_path, bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "    if v_minmax is None:\n",
    "        v_minmax = (att_map_scaled.min(), att_map_scaled.max())\n",
    "        return v_minmax\n",
    "    \n",
    "def attention_ratio(att_map, gt_tokens):\n",
    "    relevant_attention = sum([att_map[tuple(token)] for token in gt_tokens])\n",
    "    average_attention = att_map.sum() / NUM_IMG_TOKENS * len(gt_tokens)\n",
    "    return relevant_attention / (average_attention + 1e-8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(image_path, question=None):\n",
    "    if not question:\n",
    "        question = 'Write a general description of the image. Answer the question using a single word or phrase.'\n",
    "    model_output, input_ids, response_qs = bot.inference(question, image_path, return_att_map=True)\n",
    "    return model_output, input_ids, response_qs\n",
    "\n",
    "def get_attention_map(model_output, input_ids, layers, Q=False, layer_heads=None):\n",
    "    input_ids = input_ids[0].cpu()\n",
    "    index = torch.where(input_ids==-200)[0]\n",
    "    \n",
    "    att_maps = {}\n",
    "\n",
    "    if layer_heads is not None:\n",
    "        for layer_head in layer_heads:\n",
    "            layer, head = layer_head[0], layer_head[1]\n",
    "            if Q:\n",
    "                att_maps[(layer, head)] = model_output['attentions'][layer][0, head, index+NUM_IMG_TOKENS:, index:index+NUM_IMG_TOKENS].mean(dim=(0)).to(torch.float32).detach().cpu().numpy().reshape(PATCHES, PATCHES)\n",
    "            else:\n",
    "                att_maps[(layer, head)] = model_output['attentions'][layer][0, head, -1, index:index+NUM_IMG_TOKENS].to(torch.float32).detach().cpu().numpy().reshape(PATCHES, PATCHES)\n",
    "    else:\n",
    "        for layer in layers:\n",
    "            if Q:\n",
    "                att_maps[layer] = model_output['attentions'][layer][0, :, index+NUM_IMG_TOKENS:, index:index+NUM_IMG_TOKENS].mean(dim=(0,1)).to(torch.float32).detach().cpu().numpy().reshape(PATCHES, PATCHES)\n",
    "\n",
    "            else:\n",
    "                att_maps[layer] = model_output['attentions'][layer][0, :, -1, index:index+NUM_IMG_TOKENS].mean(dim=0).to(torch.float32).detach().cpu().numpy().reshape(PATCHES, PATCHES)\n",
    "        \n",
    "    return att_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_Attention_Maps(json_path, output_filename, Q=True, norm=True, layer_heads=None, filename_suffix=\"\"):\n",
    "\n",
    "    with open(f'{json_path}.json', 'r') as f:\n",
    "        qa_set = json.load(f)\n",
    "\n",
    "    qa_set_attention_maps = {}\n",
    "    for qa in tqdm(qa_set):\n",
    "        id = qa['qid']\n",
    "        prompt = f\"{qa['question']} Answer the question using a single word or phrase.\"\n",
    "        image_path = os.path.join(data_path, qa['image_name'])\n",
    "\n",
    "        # 1st pass\n",
    "        model_output, input_ids, _ = model_inference(image_path, prompt)\n",
    "        att_maps = get_attention_map(model_output, input_ids, layers=[l for l in range(28)], Q=Q, layer_heads=layer_heads)\n",
    "\n",
    "        if norm:\n",
    "            model_general_output, input_general_ids, _ = model_inference(image_path)\n",
    "            # general_att_maps = get_attention_map(model_general_output, input_general_ids, layers=[l for l in range(28)], Q=Q, layer_heads=layer_heads)\n",
    "            general_att_maps = get_attention_map(model_general_output, input_general_ids, layers=[l for l in range(28)], Q=Q, layer_heads=None)\n",
    "\n",
    "        att_map_list = {}\n",
    "        for layer in att_maps.keys():\n",
    "            att_map = att_maps[layer]\n",
    "            if norm:\n",
    "                try:\n",
    "                    general_att_map = general_att_maps[layer]\n",
    "                except:\n",
    "                    general_att_map = general_att_maps[layer[0]]\n",
    "                att_map = att_map / general_att_map\n",
    "\n",
    "            att_map_list[layer] = {\n",
    "                'att_map': att_map.tolist()\n",
    "            }\n",
    "\n",
    "        qa[\"input_ids\"] = input_ids[0].cpu().tolist()\n",
    "        qa[\"attention_maps\"] = att_map_list\n",
    "        qa_set_attention_maps[id] = qa\n",
    "\n",
    "    os.makedirs(\"./attention_maps\", exist_ok=True)\n",
    "\n",
    "    np.save(f'./attention_maps/{output_filename}_set_attention_maps{filename_suffix}.npy', qa_set_attention_maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_Filtering_KnockOut_acc(json_path, output_filename, filename_suffix, masked_layer, percentile, magnitude_threshold, att_map_layer, layer_heads=None):\n",
    "    print(f\"Loading attention maps from './attention_maps/{output_filename}_set_attention_maps{filename_suffix}.npy'\")\n",
    "    qa_set_attention_maps = np.load(f'./attention_maps/{output_filename}_set_attention_maps{filename_suffix}.npy', allow_pickle=True).item()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    incorrect_ids_qa_ak=[]\n",
    "\n",
    "    for id, qa in tqdm(qa_set_attention_maps.items()):\n",
    "        prompt = f\"{qa['question']} Answer the question using a single word or phrase.\"\n",
    "        \n",
    "        # 2nd pass\n",
    "        input_ids = np.array(qa[\"input_ids\"])\n",
    "        image_start_token_index = np.where(input_ids==-200)[0][0]\n",
    "        att_map_list = qa[\"attention_maps\"]\n",
    "\n",
    "        image_path = os.path.join(data_path, qa['image_name'])\n",
    "\n",
    "        last_token_index = [idx for idx in range(image_start_token_index + NUM_IMG_TOKENS, input_ids.shape[-1] + NUM_IMG_TOKENS - 1)]\n",
    "\n",
    "        if not isinstance(last_token_index, list):\n",
    "            last_token_index = [last_token_index]\n",
    "\n",
    "        block_config = {}\n",
    "\n",
    "        if layer_heads is not None:\n",
    "            att_map = []\n",
    "            for layer_head in layer_heads:\n",
    "                layer, head = layer_head[0], layer_head[1]\n",
    "                att_map.append(att_map_list[(layer, head)]['att_map'])\n",
    "            att_map = np.array(att_map).mean(axis=0)\n",
    "        else:\n",
    "            att_map = np.array(att_map_list[att_map_layer]['att_map'])\n",
    "\n",
    "        flattened_att_map = att_map.flatten()\n",
    "        if percentile is not None:\n",
    "            threshold = np.percentile(flattened_att_map, percentile)\n",
    "            low_attention_indices = np.where(flattened_att_map < threshold)[0]\n",
    "        elif magnitude_threshold is not None:\n",
    "            threshold = magnitude_threshold\n",
    "            flattened_att_map = normalize(flattened_att_map.copy())\n",
    "            low_attention_indices = np.where(flattened_att_map < threshold)[0]\n",
    "            \n",
    "\n",
    "        image_range = [image_start_token_index + patch_index for patch_index in low_attention_indices]\n",
    "\n",
    "\n",
    "        block_ids = [image_range, last_token_index]\n",
    "        temp = [(stok1, stok0) for stok0 in block_ids[0] for stok1 in block_ids[1]]\n",
    "\n",
    "\n",
    "        for layer in masked_layer:\n",
    "            block_config[layer] = copy.deepcopy(temp)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            block_attn_hooks = set_block_attn_hooks_llava(bot.model, block_config, block_desc=\"Image->Question\")\n",
    "            _, _, response_qs = bot.inference(prompt, image_path)\n",
    "            remove_wrapper_llava(bot.model, block_attn_hooks)\n",
    "            \n",
    "        answer = qa['answer'].capitalize()\n",
    "\n",
    "        clean_response = response_qs.replace(\"<|endoftext|>\", \"\").capitalize()\n",
    "\n",
    "        if clean_response in answer or answer in clean_response:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect_ids_qa_ak.append(id)\n",
    "        total += 1\n",
    "\n",
    "    print(f\"OG Accuracy on {json_path.replace('_', ' ')}: {(correct / total) * 100:.4f}%, {correct}/{total}\")\n",
    "\n",
    "    os.makedirs(\"./incorrect\", exist_ok=True)\n",
    "    with open(f\"./incorrect/incorrect_ids_{output_filename}_ak{filename_suffix}.json\", \"w\") as f:\n",
    "        json.dump(incorrect_ids_qa_ak, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading attention maps from './attention_maps/RAD_set_attention_maps_Q_Norm.npy'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1297 [00:00<?, ?it/s]Qwen2Model is using Qwen2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "100%|██████████| 1297/1297 [08:43<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG Accuracy on /home/user/Desktop/DiskSDA/dataset/VQA RAD/filter/sampled data: 68.6970%, 891/1297\n"
     ]
    }
   ],
   "source": [
    "Q = False\n",
    "norm = True\n",
    "masked_layer = [16]\n",
    "percentile = 50\n",
    "magnitude_threshold = None\n",
    "att_map_layer = 16\n",
    "layer_heads = None\n",
    "\n",
    "layer_heads = [[15, 4], [16, 5], [15, 1], [10, 6], [4, 0], [15, 24], [19, 6], [8, 3], [19, 18], [18, 10], [4, 17], [16, 2], [16, 17], [16, 1], [14, 23], [6, 18], [18, 19], [6, 11], [0, 2], [17, 26]]\n",
    "\n",
    "\n",
    "filename_suffix = \"\"\n",
    "if Q:\n",
    "    filename_suffix += \"_Q\"\n",
    "else:\n",
    "    filename_suffix += \"_Last\"\n",
    "if norm:\n",
    "    filename_suffix += \"_Norm\"\n",
    "else:\n",
    "    filename_suffix += \"_woNorm\"\n",
    "if layer_heads is not None:\n",
    "    if len(layer_heads) > 6:\n",
    "        filename_suffix += f\"_head_top{len(layer_heads)}\"\n",
    "    else:\n",
    "        filename_suffix += f\"_head{layer_heads}\"\n",
    "\n",
    "\n",
    "data_path = \"\"\n",
    "question_file = \"\"\n",
    "output_filename = \"RAD\"\n",
    "\n",
    "get_Attention_Maps(question_file, output_filename, Q=Q, norm=norm, layer_heads=layer_heads, filename_suffix=filename_suffix)\n",
    "get_Filtering_KnockOut_acc(question_file, output_filename, filename_suffix, masked_layer=masked_layer, percentile=percentile, magnitude_threshold=magnitude_threshold, att_map_layer=att_map_layer, layer_heads=layer_heads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
