{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cli_visualization import HuatuoChatbot\n",
    "import torch\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import TextStreamer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "NUM_IMG_TOKENS = 576\n",
    "PATCHES = 24\n",
    "SIZE = (336, 336)\n",
    "\n",
    "bot = HuatuoChatbot(\"FreedomIntelligence/HuatuoGPT-Vision-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_attention_maps(question, image_path, layers=range(28)):\n",
    "\n",
    "    general_question = 'Write a general description of the image.'\n",
    "    prompt = f\"{question} Answer the question using a single word or phrase.\"\n",
    "    general_prompt = f\"{general_question} Answer the question using a single word or phrase.\"\n",
    "\n",
    "    model_output, input_ids = bot.inference_with_attention_output(prompt,image_path)\n",
    "    input_ids = input_ids[0].cpu()\n",
    "    index = torch.where(input_ids==-200)[0]\n",
    "    att_maps = np.array([model_output['attentions'][layer][0, :, -1, index:index+NUM_IMG_TOKENS].mean(dim=0).to(torch.float32).detach().cpu().numpy() for layer in layers])\n",
    "\n",
    "    model_output, input_ids = bot.inference_with_attention_output(general_prompt,image_path)\n",
    "    input_ids = input_ids[0].cpu()\n",
    "    index = torch.where(input_ids==-200)[0]\n",
    "    general_att_maps = np.array([model_output['attentions'][layer][0, :, -1, index:index+NUM_IMG_TOKENS].mean(dim=0).to(torch.float32).detach().cpu().numpy() for layer in layers])\n",
    "\n",
    "    return att_maps, general_att_maps\n",
    "\n",
    "\n",
    "def attention_ratio_vectorized(visual_attentions, gt_tokens):\n",
    "\n",
    "    token_indices = np.array([token[1] * PATCHES + token[0] for token in gt_tokens])\n",
    "    relevant_attention = np.sum(visual_attentions[..., token_indices], axis=-1)\n",
    "    average_attention = np.sum(visual_attentions, axis=-1) / NUM_IMG_TOKENS * len(token_indices)\n",
    "\n",
    "    return relevant_attention / (average_attention + 1e-8)\n",
    "\n",
    "\n",
    "def js_divergence_vectorized(att_map, gt_tokens, epsilon=1e-8):\n",
    "\n",
    "    att_map = att_map / (att_map.sum(axis=-1, keepdims=True) + epsilon)\n",
    "\n",
    "    gt_mask = np.zeros(att_map.shape[-1])\n",
    "    for token in gt_tokens:\n",
    "        gt_mask[token[1] * PATCHES + token[0]] = 1\n",
    "    gt_mask = gt_mask / gt_mask.sum()\n",
    "    \n",
    "    m = 0.5 * (att_map + gt_mask) \n",
    "    kl_att_m = np.sum(att_map * np.log((att_map + epsilon) / (m + epsilon)), axis=-1)\n",
    "    kl_gt_m = np.sum(gt_mask * np.log((gt_mask + epsilon) / (m + epsilon)), axis=-1)\n",
    "    js_div = 0.5 * (kl_att_m + kl_gt_m)\n",
    "\n",
    "    return js_div\n",
    "\n",
    "\n",
    "def kl_divergence_vectorized(att_map, gt_tokens):\n",
    "\n",
    "    att_map = att_map / att_map.sum()\n",
    "\n",
    "    gt_mask = np.zeros(att_map.shape[-1])\n",
    "    for token in gt_tokens:\n",
    "        gt_mask[token[1] * PATCHES + token[0]] = 1\n",
    "    gt_mask = gt_mask / gt_mask.sum()\n",
    "\n",
    "    epsilon = 1e-12\n",
    "    att_map = np.clip(att_map, epsilon, 1)\n",
    "    gt_mask = np.clip(gt_mask, epsilon, 1)\n",
    "\n",
    "    return np.sum(gt_mask * np.log(gt_mask / att_map), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLAKE Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_ratios_normalized = []\n",
    "attention_kl_normalized = []\n",
    "attention_js_normalized = []\n",
    "questions = []\n",
    "input_path = \"./VGMED_localization_questions.jsonl\"\n",
    "with open(input_path, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        questions.append(json.loads(line))\n",
    "\n",
    "for sample in tqdm(questions):\n",
    "    image_path = os.path.join(\"../LLaVA/playground/data/Slake1.0/imgs\", sample[\"image\"], \"source.jpg\")\n",
    "    qs = sample[\"question\"]\n",
    "    att_maps, general_att_maps = generate_attention_maps(qs, image_path)\n",
    "    gt_tokens = sample[\"gt_tokens\"]\n",
    "\n",
    "    att_ratio_normalized = attention_ratio_vectorized(att_maps/general_att_maps, gt_tokens)\n",
    "    att_kl_normalized = kl_divergence_vectorized(att_maps/general_att_maps, gt_tokens)\n",
    "    att_js_normalized = js_divergence_vectorized(att_maps/general_att_maps, gt_tokens)\n",
    "    \n",
    "    attention_ratios_normalized.append(att_ratio_normalized)\n",
    "    attention_kl_normalized.append(att_kl_normalized)\n",
    "    attention_js_normalized.append(att_js_normalized)\n",
    "\n",
    "\n",
    "torch.save(attention_ratios_normalized, \"attention_measurement/VGMED_LOCAL_attention_ratios_normalized.pt\")\n",
    "torch.save(attention_kl_normalized, \"attention_measurement/VGMED_LOCAL_attention_kl_normalized.pt\")\n",
    "torch.save(attention_js_normalized, \"attention_measurement/VGMED_LOCAL_attention_js_normalized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_ratios_normalized = torch.load(\"attention_measurement/VGMED_LOCAL_attention_ratios_normalized.pt\")\n",
    "attention_kl_normalized = torch.load(\"attention_measurement/VGMED_LOCAL_attention_kl_normalized.pt\")\n",
    "attention_js_normalized = torch.load(\"attention_measurement/VGMED_LOCAL_attention_js_normalized.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ratios = np.array(attention_ratios_normalized).mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(28), layer_ratios, marker='o', linewidth=2)\n",
    "\n",
    "# Add data labels to each point\n",
    "for x, y in zip(range(28), layer_ratios):\n",
    "    plt.text(x, y+0.001, f\"{y:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Attention Ratio by Layer\", fontsize=14)\n",
    "plt.xlabel(\"Model Layers\", fontsize=12)\n",
    "plt.ylabel(\"Average Attention Ratio\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xticks(range(28))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ratios = np.array(attention_kl_normalized).mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(28), layer_ratios, marker='o', linewidth=2)\n",
    "\n",
    "# Add data labels to each point\n",
    "for x, y in zip(range(28), layer_ratios):\n",
    "    plt.text(x, y+0.001, f\"{y:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Attention KL Divergence by Layer\", fontsize=14)\n",
    "plt.xlabel(\"Model Layers\", fontsize=12)\n",
    "plt.ylabel(\"Average Attention KL Divergence\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xticks(range(28))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ratios = np.array(attention_js_normalized).mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(28), layer_ratios, marker='o', linewidth=2)\n",
    "\n",
    "# Add data labels to each point\n",
    "for x, y in zip(range(28), layer_ratios):\n",
    "    plt.text(x, y+0.001, f\"{y:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Attention JS Divergence by Layer\", fontsize=14)\n",
    "plt.xlabel(\"Model Layers\", fontsize=12)\n",
    "plt.ylabel(\"Average Attention JS Divergence\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xticks(range(28))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLAKE Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_ratios_normalized = []\n",
    "attention_kl_normalized = []\n",
    "attention_js_normalized = []\n",
    "questions = []\n",
    "input_path = \"./VGMED_attribute_questions.jsonl\"\n",
    "with open(input_path, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        questions.append(json.loads(line))\n",
    "\n",
    "for sample in tqdm(questions):\n",
    "    image_path = os.path.join(\"../LLaVA/playground/data/Slake1.0/imgs\", sample[\"image\"], \"source.jpg\")\n",
    "    qs = sample[\"question\"]\n",
    "    att_maps, general_att_maps = generate_attention_maps(qs, image_path)\n",
    "    gt_tokens = sample[\"gt_tokens\"]\n",
    "\n",
    "    att_ratio_normalized = attention_ratio_vectorized(att_maps/general_att_maps, gt_tokens)\n",
    "    att_kl_normalized = kl_divergence_vectorized(att_maps/general_att_maps, gt_tokens)\n",
    "    att_js_normalized = js_divergence_vectorized(att_maps/general_att_maps, gt_tokens)\n",
    "    \n",
    "    attention_ratios_normalized.append(att_ratio_normalized)\n",
    "    attention_kl_normalized.append(att_kl_normalized)\n",
    "    attention_js_normalized.append(att_js_normalized)\n",
    "\n",
    "torch.save(attention_ratios_normalized, \"attention_measurement/VGMED_ATTR_attention_ratios_normalized.pt\")\n",
    "torch.save(attention_kl_normalized, \"attention_measurement/VGMED_ATTR_attention_kl_normalized.pt\")\n",
    "torch.save(attention_js_normalized, \"attention_measurement/VGMED_ATTR_attention_js_normalized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_ratios_normalized = torch.load(\"attention_measurement/VGMED_ATTR_attention_ratios_normalized.pt\")\n",
    "attention_kl_normalized = torch.load(\"attention_measurement/VGMED_ATTR_attention_kl_normalized.pt\")\n",
    "attention_js_normalized = torch.load(\"attention_measurement/VGMED_ATTR_attention_js_normalized.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ratios = np.array(attention_ratios_normalized).mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(28), layer_ratios, marker='o', linewidth=2)\n",
    "\n",
    "# Add data labels to each point\n",
    "for x, y in zip(range(28), layer_ratios):\n",
    "    plt.text(x, y+0.001, f\"{y:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Attention Ratio by Layer\", fontsize=14)\n",
    "plt.xlabel(\"Model Layers\", fontsize=12)\n",
    "plt.ylabel(\"Average Attention Ratio\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xticks(range(28))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ratios = np.array(attention_kl_normalized).mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(28), layer_ratios, marker='o', linewidth=2)\n",
    "\n",
    "# Add data labels to each point\n",
    "for x, y in zip(range(28), layer_ratios):\n",
    "    plt.text(x, y+0.001, f\"{y:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Attention KL Divergence by Layer\", fontsize=14)\n",
    "plt.xlabel(\"Model Layers\", fontsize=12)\n",
    "plt.ylabel(\"Average Attention KL Divergence\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xticks(range(28))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ratios = np.array(attention_js_normalized).mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(28), layer_ratios, marker='o', linewidth=2)\n",
    "\n",
    "# Add data labels to each point\n",
    "for x, y in zip(range(28), layer_ratios):\n",
    "    plt.text(x, y+0.001, f\"{y:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Attention JS Divergence by Layer\", fontsize=14)\n",
    "plt.xlabel(\"Model Layers\", fontsize=12)\n",
    "plt.ylabel(\"Average Attention JS Divergence\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xticks(range(28))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_ratios_normalized = []\n",
    "attention_kl_normalized = []\n",
    "attention_js_normalized = []\n",
    "questions = []\n",
    "input_path = \"./COCO_localization_questions_previous.jsonl\"\n",
    "with open(input_path, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        questions.append(json.loads(line))\n",
    "\n",
    "for sample in tqdm(questions):\n",
    "    image_path = os.path.join('val2014', f'COCO_val2014_{sample[\"image\"]:012d}.jpg')\n",
    "    qs = sample[\"question\"]\n",
    "    att_maps, general_att_maps = generate_attention_maps(qs, image_path)\n",
    "    gt_tokens = sample[\"gt_tokens\"]\n",
    "\n",
    "    att_ratio_normalized = attention_ratio_vectorized(att_maps/general_att_maps, gt_tokens)\n",
    "    att_kl_normalized = kl_divergence_vectorized(att_maps/general_att_maps, gt_tokens)\n",
    "    att_js_normalized = js_divergence_vectorized(att_maps/general_att_maps, gt_tokens)\n",
    "    \n",
    "    attention_ratios_normalized.append(att_ratio_normalized)\n",
    "    attention_kl_normalized.append(att_kl_normalized)\n",
    "    attention_js_normalized.append(att_js_normalized)\n",
    "\n",
    "torch.save(attention_ratios_normalized, \"attention_measurement/COCO_LOCAL_attention_ratios_normalized.pt\")\n",
    "torch.save(attention_kl_normalized, \"attention_measurement/COCO_LOCAL_attention_kl_normalized.pt\")\n",
    "torch.save(attention_js_normalized, \"attention_measurement/COCO_LOCAL_attention_js_normalized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_ratios_normalized = torch.load(\"attention_measurement/COCO_LOCAL_attention_ratios_normalized.pt\")\n",
    "attention_kl_normalized = torch.load(\"attention_measurement/COCO_LOCAL_attention_kl_normalized.pt\")\n",
    "attention_js_normalized = torch.load(\"attention_measurement/COCO_LOCAL_attention_js_normalized.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ratios = np.array(attention_ratios_normalized).mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(28), layer_ratios, marker='o', linewidth=2)\n",
    "\n",
    "# Add data labels to each point\n",
    "for x, y in zip(range(28), layer_ratios):\n",
    "    plt.text(x, y+0.001, f\"{y:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Attention Ratio by Layer\", fontsize=14)\n",
    "plt.xlabel(\"Model Layers\", fontsize=12)\n",
    "plt.ylabel(\"Average Attention Ratio\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xticks(range(28))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ratios = np.array(attention_kl_normalized).mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(28), layer_ratios, marker='o', linewidth=2)\n",
    "\n",
    "# Add data labels to each point\n",
    "for x, y in zip(range(28), layer_ratios):\n",
    "    plt.text(x, y+0.001, f\"{y:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Attention KL Divergence by Layer\", fontsize=14)\n",
    "plt.xlabel(\"Model Layers\", fontsize=12)\n",
    "plt.ylabel(\"Average Attention KL Divergence\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xticks(range(28))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ratios = np.array(attention_js_normalized).mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(28), layer_ratios, marker='o', linewidth=2)\n",
    "\n",
    "# Add data labels to each point\n",
    "for x, y in zip(range(28), layer_ratios):\n",
    "    plt.text(x, y+0.001, f\"{y:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Attention JS Divergence by Layer\", fontsize=14)\n",
    "plt.xlabel(\"Model Layers\", fontsize=12)\n",
    "plt.ylabel(\"Average Attention JS Divergence\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xticks(range(28))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_ratios_normalized = []\n",
    "attention_kl_normalized = []\n",
    "attention_js_normalized = []\n",
    "questions = []\n",
    "input_path = \"./COCO_attribute_questions.jsonl\"\n",
    "with open(input_path, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        questions.append(json.loads(line))\n",
    "\n",
    "for sample in tqdm(questions):\n",
    "    image_path = os.path.join('val2014', f'COCO_val2014_{sample[\"image\"]:012d}.jpg')\n",
    "    qs = sample[\"question\"]\n",
    "    att_maps, general_att_maps = generate_attention_maps(qs, image_path)\n",
    "    gt_tokens = sample[\"gt_tokens\"]\n",
    "\n",
    "    att_ratio_normalized = attention_ratio_vectorized(att_maps/general_att_maps, gt_tokens)\n",
    "    att_kl_normalized = kl_divergence_vectorized(att_maps/general_att_maps, gt_tokens)\n",
    "    att_js_normalized = js_divergence_vectorized(att_maps/general_att_maps, gt_tokens)\n",
    "    \n",
    "    attention_ratios_normalized.append(att_ratio_normalized)\n",
    "    attention_kl_normalized.append(att_kl_normalized)\n",
    "    attention_js_normalized.append(att_js_normalized)\n",
    "\n",
    "torch.save(attention_ratios_normalized, \"attention_measurement/COCO_ATTR_attention_ratios_normalized.pt\")\n",
    "torch.save(attention_kl_normalized, \"attention_measurement/COCO_ATTR_attention_kl_normalized.pt\")\n",
    "torch.save(attention_js_normalized, \"attention_measurement/COCO_ATTR_attention_js_normalized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_ratios_normalized = torch.load(\"attention_measurement/COCO_ATTR_attention_ratios_normalized.pt\")\n",
    "attention_kl_normalized = torch.load(\"attention_measurement/COCO_ATTR_attention_kl_normalized.pt\")\n",
    "attention_js_normalized = torch.load(\"attention_measurement/COCO_ATTR_attention_js_normalized.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ratios = np.array(attention_ratios_normalized).mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(28), layer_ratios, marker='o', linewidth=2)\n",
    "\n",
    "# Add data labels to each point\n",
    "for x, y in zip(range(28), layer_ratios):\n",
    "    plt.text(x, y+0.001, f\"{y:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Attention Ratio by Layer\", fontsize=14)\n",
    "plt.xlabel(\"Model Layers\", fontsize=12)\n",
    "plt.ylabel(\"Average Attention Ratio\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xticks(range(28))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ratios = np.array(attention_kl_normalized).mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(28), layer_ratios, marker='o', linewidth=2)\n",
    "\n",
    "# Add data labels to each point\n",
    "for x, y in zip(range(28), layer_ratios):\n",
    "    plt.text(x, y+0.001, f\"{y:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Attention KL Divergence by Layer\", fontsize=14)\n",
    "plt.xlabel(\"Model Layers\", fontsize=12)\n",
    "plt.ylabel(\"Average Attention KL Divergence\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xticks(range(28))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ratios = np.array(attention_js_normalized).mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(28), layer_ratios, marker='o', linewidth=2)\n",
    "\n",
    "# Add data labels to each point\n",
    "for x, y in zip(range(28), layer_ratios):\n",
    "    plt.text(x, y+0.001, f\"{y:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Attention JS Divergence by Layer\", fontsize=14)\n",
    "plt.xlabel(\"Model Layers\", fontsize=12)\n",
    "plt.ylabel(\"Average Attention JS Divergence\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xticks(range(28))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
